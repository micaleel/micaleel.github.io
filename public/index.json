[{"content":"Monitoring and observability systems track the performance and reliability of recommender systems in production. They report metrics computed from model inputs, outputs, and customer interactions. These metrics help organisations assess the health and effectiveness of their recommender systems.\nFailure to adopt a proper monitoring and observability system could negatively impact business operations affecting things like user experience and revenue.\nI believe there are three main components essential for monitoring a recommender system in production: Metrics, Logging, and Reporting.\nMetrics: Measuring Performance and Business Alignment Metrics are the backbone of any ML monitoring system because they provide quantifiable insights into model performance and business impact.\nFor recommender systems, a good starting point is to consider metrics that are commonly used to evaluate retrieval and ranking systems. For example:\nAccuracy metrics: NDCG (Normalised Discounted Cumulative Gain), MAP (Mean Average Precision), or MRR (Mean Reciprocal Rank) to evaluate ranking quality. Engagement metrics: Click-through rate (CTR), conversion rate, or time spent on recommended items. Diversity metrics: Category coverage or variance in item similarity scores (to ensure a varied recommendation set). Business KPIs: Revenue or cost per session, or estimated quality of items that delighted the user. Tracking trends and anomalies in metrics could help flag performance degradation or misalignment with business goals.\nLogging: Capturing Inference Data for Analysis Comprehensive logging is crucial for post-hoc analysis, drift detection, and debugging.\nFor recommender systems, consider logging:\nInput features: User demographics, historical interactions, and contextual information. Model outputs: Raw scores, rankings, and final recommendations. Serving metadata: Timestamp, model version, and model-serving API version. User interactions: Clicks, consumptions, or other engagement signals. This data facilitates the reconstruction the model\u0026rsquo;s decision-making process, detect data drift, and identify patterns in errors or unexpected behaviours.\nReporting: Visualising Trends and Anomalies Effective reporting transforms raw data into actionable insights. For recommender systems, consider these reporting strategies:\nGlobal performance dashboards: Visualise key metrics over time, highlighting trends and anomalies. Segment-level analysis: Break down performance by user segments, item categories, or geographical regions. User-level reports: Enable deep dives into individual user experiences for troubleshooting or personalisation refinement. Drift monitoring: Visualise changes in feature distributions or model predictions over time. A/B test results: Compare metrics across different model versions or recommendation strategies. These reports should be designed to trigger appropriate actions, whether it\u0026rsquo;s retraining the model, adjusting business rules, or investigating specific issues.\nConclusion By implementing a robust ML monitoring and observability system, organisations can ensure their recommender systems remain reliable and aligned with business objectives. This approach enables proactive maintenance, faster troubleshooting, and data-driven decision-making, ultimately leading to improved user satisfaction and business outcomes.\nThe key to success lies in choosing the right metrics, logging comprehensive data, and creating insightful reports that drive action.\n","permalink":"https://micaleel.github.io/posts/monitoring-recsys/","summary":"Monitoring and observability systems track the performance and reliability of recommender systems in production. They report metrics computed from model inputs, outputs, and customer interactions. These metrics help organisations assess the health and effectiveness of their recommender systems.\nFailure to adopt a proper monitoring and observability system could negatively impact business operations affecting things like user experience and revenue.\nI believe there are three main components essential for monitoring a recommender system in production: Metrics, Logging, and Reporting.","title":"Monitoring Recommenders"},{"content":"The promise of generative AI is exaggerated in the short-term and underestimated in the long-term leading to an uncontrollable hype.\nNaturally, many choose to ride this hype with false advertisement to further their agenda — whether it is to drive up stock prices or to win customers.\nBBC recently referred this false misrepresentation of the benefits of AI (to businesses) as \u0026ldquo;AI washing\u0026rdquo;:\nIt is a problem that has quietly existed for a number of years, according to data from another tech investment firm, MMC Ventures. In a 2019 study it found that 40% of new tech firms that described themselves as \u0026ldquo;AI start-ups\u0026rdquo; in fact used virtually no AI at all.\nAmazon itself decieved customers into thinking that their \u0026ldquo;Just Walk Out\u0026rdquo; technology — that powered Amazon Fresh and Amazon Go shops — was primarly driven by AI. It turned out that the system was supported by hundreds of works manually checking over 70% of the transactions.\nWhile should eventually expect regulation to tame the false promises of AI to customer, in the short term we must adapt to ridiculous ads like an AI driven toothbrush.\nPeople charged with developing AI solutions have the impossible task to taming expectations and disabusing the believe the stakeholder have about AI being a silverbullet.\n","permalink":"https://micaleel.github.io/posts/ai-washing/","summary":"The promise of generative AI is exaggerated in the short-term and underestimated in the long-term leading to an uncontrollable hype.\nNaturally, many choose to ride this hype with false advertisement to further their agenda — whether it is to drive up stock prices or to win customers.\nBBC recently referred this false misrepresentation of the benefits of AI (to businesses) as \u0026ldquo;AI washing\u0026rdquo;:\nIt is a problem that has quietly existed for a number of years, according to data from another tech investment firm, MMC Ventures.","title":"AI Washing"},{"content":"Hi, I\u0026rsquo;m Khalil I am a Data Scientist and Machine Learning Engineer based in Dublin.\nI hold a PhD, and my work sits at the intersection of Personalisation, AI Engineering, Search, and Discovery. I\u0026rsquo;m driven by curiosity, creativity, and a relentless desire to build systems that are both intelligent and humane.\nI spend most of my time thinking about how people find the things they care about—and how we can make that faster, smarter, and more meaningful.\nOutside of building and shipping, I care deeply about how we communicate our work—because the best ideas deserve clear, elegant expression. I love to code, teach, and explore better ways to present complex ideas.\nI\u0026rsquo;m guided by a simple belief: the world gets better when we stay ambitious—without stepping on others to climb higher.\nIf any of this resonates, you’re welcome to follow along. I write about ML systems, personalization, and what I’m learning. Occasionally, I send updates via a newsletter (still figuring out the right cadence!) and love connecting with thoughtful folks across the space.\nThis blog was crafted with care and curiosity. Thanks for stopping by.\n","permalink":"https://micaleel.github.io/about/","summary":"All about the author and this blog","title":"About Me"},{"content":" Naked Money: A Revealing Look at Our Financial System ","permalink":"https://micaleel.github.io/bookshelf/","summary":" Naked Money: A Revealing Look at Our Financial System ","title":"Bookshelf"}]