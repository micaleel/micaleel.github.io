<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Observability on micaleel&#39;s notes</title>
    <link>https://micaleel.github.io/tags/observability/</link>
    <description>Recent content in Observability on micaleel&#39;s notes</description>
    <generator>Hugo -- 0.127.0</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 16 Apr 2025 20:07:36 +0100</lastBuildDate>
    <atom:link href="https://micaleel.github.io/tags/observability/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Monitoring Recommenders</title>
      <link>https://micaleel.github.io/posts/monitoring-recsys/</link>
      <pubDate>Mon, 01 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://micaleel.github.io/posts/monitoring-recsys/</guid>
      <description>Monitoring and observability systems track the performance and reliability of recommender systems in production. They report metrics computed from model inputs, outputs, and customer interactions. These metrics help organisations assess the health and effectiveness of their recommender systems.
Failure to adopt a proper monitoring and observability system could negatively impact business operations affecting things like user experience and revenue.
I believe there are three main components essential for monitoring a recommender system in production: Metrics, Logging, and Reporting.</description>
      <content:encoded><![CDATA[<p>Monitoring and observability systems track the performance and reliability of recommender systems in production. They report metrics computed from  model inputs, outputs, and customer interactions. These metrics help organisations assess the health and effectiveness of their recommender systems.</p>
<p>Failure to adopt a proper monitoring and observability system could negatively impact business operations affecting things like user experience and revenue.</p>
<p>I believe there are three main components essential for monitoring a recommender system in production: Metrics, Logging, and Reporting.</p>
<h3 id="metrics-measuring-performance-and-business-alignment">Metrics: Measuring Performance and Business Alignment</h3>
<p>Metrics are the backbone of any ML monitoring system because they provide quantifiable insights into model performance and business impact.</p>
<p>For recommender systems, a good starting point is to consider metrics that are commonly used to evaluate retrieval and ranking systems. For example:</p>
<ul>
<li><strong>Accuracy metrics</strong>: NDCG (Normalised Discounted Cumulative Gain), MAP (Mean Average Precision), or MRR (Mean Reciprocal Rank) to evaluate ranking quality.</li>
<li><strong>Engagement metrics</strong>: Click-through rate (CTR), conversion rate, or time spent on recommended items.</li>
<li><strong>Diversity metrics</strong>: Category coverage or variance in item similarity scores (to ensure a varied recommendation set).</li>
<li><strong>Business KPIs</strong>: Revenue or cost per session, or estimated quality of items that delighted the user.</li>
</ul>
<p>Tracking trends and anomalies in metrics could help flag performance degradation or misalignment with business goals.</p>
<h3 id="logging-capturing-inference-data-for-analysis">Logging: Capturing Inference Data for Analysis</h3>
<p>Comprehensive logging is crucial for post-hoc analysis, drift detection, and debugging.</p>
<p>For recommender systems, consider logging:</p>
<ul>
<li><strong>Input features</strong>: User demographics, historical interactions, and contextual information.</li>
<li><strong>Model outputs</strong>: Raw scores, rankings, and final recommendations.</li>
<li><strong>Serving metadata</strong>: Timestamp, model version, and model-serving API version.</li>
<li><strong>User interactions</strong>: Clicks, consumptions, or other engagement signals.</li>
</ul>
<p>This data facilitates the reconstruction the model&rsquo;s decision-making process, detect data drift, and identify patterns in errors or unexpected behaviours.</p>
<h3 id="reporting-visualising-trends-and-anomalies">Reporting: Visualising Trends and Anomalies</h3>
<p>Effective reporting transforms raw data into actionable insights. For recommender systems, consider these reporting strategies:</p>
<ul>
<li><strong>Global performance dashboards</strong>: Visualise key metrics over time, highlighting trends and anomalies.</li>
<li><strong>Segment-level analysis</strong>: Break down performance by user segments, item categories, or geographical regions.</li>
<li><strong>User-level reports</strong>: Enable deep dives into individual user experiences for troubleshooting or personalisation refinement.</li>
<li><strong>Drift monitoring</strong>: Visualise changes in feature distributions or model predictions over time.</li>
<li><strong>A/B test results</strong>: Compare metrics across different model versions or recommendation strategies.</li>
</ul>
<p>These reports should be designed to trigger appropriate actions, whether it&rsquo;s retraining the model, adjusting business rules, or investigating specific issues.</p>
<h3 id="conclusion">Conclusion</h3>
<p>By implementing a robust ML monitoring and observability system, organisations can ensure their recommender systems remain reliable and aligned with business objectives. This approach enables proactive maintenance, faster troubleshooting, and data-driven decision-making, ultimately leading to improved user satisfaction and business outcomes.</p>
<p>The key to success lies in choosing the right metrics, logging comprehensive data, and creating insightful reports that drive action.</p>
]]></content:encoded>
    </item>
  </channel>
</rss>
